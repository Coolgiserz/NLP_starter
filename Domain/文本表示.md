# 文本表示

- 文本表示的目的是什么？
- 发展过程如何？
- 当前文本表示的挑战在哪？

## 简介

### 为什么要研究文本表示

现实世界和网络世界中有包含大量的文本数据，我们可以通过计算机处理分析这些数据以创造价值。如从海量非结构化文本数据中抽取信息以构造搜索引擎或者知识图谱，对新闻内容进行自动分类以减少人工作业量，(^-^)。

但要通过计算机来处理这些数据首先需要将文本转换成可运算的数学表示。这便是文本表示研究的起点。

### 文本表示研究经历了哪些历程

####独热编码

最简单的方法是独热编码，即对于词典中的每一个单词，都使用一个维度为词典大小的向量表示，其中第i个单词的特征向量的第i维度为1，其余维度均为0。这样则**每一个单词都有一个唯一的向量表示**。

但是独热编码存在很大的问题：单词之间的向量表示都是正交的。这意味着在这种向量表示下，不能够建模出单词之间的相似性，因为对两个独热编码的向量计算距离或者相似性总是一个相等的常量。而我们所使用语言中的单词存在同义词、近义词、反义词等等关系，即文本之间是可以有“相似性”和“距离”之说的，如我们可以认为“我”、“你”、“他”这一组词是相近的，“爱”、“恨”这一组词是相近的。独热编码的文本表示仅能够唯一地表示文本，却无法让我们度量这样文本相似程度的信息。

独热编码存在的另外一个问题是，容易受到维度灾难的限制，尤其是在文本语料库巨大，词典大小很大的时候，用于表示每个文本的向量很长，而其中只有一个非0元素，因此效率较低。

【注：独热编码也不局限应用在文本表示，凡是涉及到等级、类别的特征都可以转化为独热编码的形式。】

#### 分布式表示

为了解决独热编码存在的问题，有研究者提出分布式表示。

##### NNLM

##### Word2Vec

##### ELMo

##### BERT

### 文本表示还需要解决的问题

- 歧义问题
- 未登陆词
- 旧词新义

## 参考

### 论文

### 博客

- [[NLP] 秒懂词向量Word2vec的本质](https://zhuanlan.zhihu.com/p/26306795)
- [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)

### 代码仓库

- [word2vec Parameter Learning Explained](http://wiki.hacksmeta.com/static/pdf/word2vec-Parameter-Learning-Explained-5.pdf) [[demo]](https://ronxin.github.io/wevi/)

### 可视化

### 词向量

- [Chinese Word Vectors 中文词向量](https://github.com/Embedding/Chinese-Word-Vectors)

