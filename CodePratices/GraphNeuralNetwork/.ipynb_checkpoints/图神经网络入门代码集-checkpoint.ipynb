{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图神经网络入门代码集\n",
    "作者：丁雨山、彬斌\n",
    "## 实战案例：搭建影视作品的图网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载数据\n",
    "df = pd.read_csv('/home/jovyan/kernel/netflix_titles.csv')\n",
    "\n",
    "# 取出导演属性，如果该属性的值为空，则返回空列表 [] ，否则返回所有导演列表\n",
    "df['directors'] = df['director'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\n",
    "\n",
    "# 取出演员属性，如果该属性的值为空，则返回空列表 [] ，否则返回所有演员列表\n",
    "df['actors'] = df['cast'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\n",
    "\n",
    "# 取出导演属性，如果该属性的值为空，则返回空列表 [] ，否则返回所有影视类型列表\n",
    "df['categories'] = df['listed_in'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\n",
    "\n",
    "# 取出国家属性，如果该属性的值为空，则返回空列表 [] ，否则返回所有国家列表\n",
    "df['countries'] = df['country'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\n",
    "\n",
    "# 取出我们需要的5个属性\n",
    "df = df[[\"title\", \"directors\", \"actors\", \"categories\", \"countries\"]]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# 初始化一个图网络 graph network (gn)\n",
    "gn = nx.Graph(label=\"Netflix\")\n",
    "\n",
    "# 遍历数据来给图网络添加节点和边\n",
    "for i, row in df.iterrows():\n",
    "    # 添加影视节点\n",
    "    gn.add_node(row['title'], label=\"MOVIE\")\n",
    "    \n",
    "    # 遍历演员列表\n",
    "    for actor in row['actors']:\n",
    "        \n",
    "        # 添加人物节点\n",
    "        gn.add_node(actor, label=\"PERSON\")\n",
    "        \n",
    "        # 添加该人物与该影视之间的边，关系为 ACTED_IN\n",
    "        gn.add_edge(row['title'], actor, label=\"ACTED_IN\")\n",
    "    \n",
    "    # 遍历导演列表\n",
    "    for director in row['directors']:\n",
    "        \n",
    "        # 添加人物节点\n",
    "        gn.add_node(director, label=\"PERSON\")\n",
    "        \n",
    "        # 添加该人物与该影视之间的边， 关系为 DERECTED_BY\n",
    "        gn.add_edge(row['title'], director, label=\"DERECTED_BY\")\n",
    "    \n",
    "    # 遍历影视类型列表\n",
    "    for cat in row['categories']:\n",
    "        \n",
    "        # 添加影视类型节点\n",
    "        gn.add_node(cat, label=\"CATEGORY\")\n",
    "        \n",
    "        # 添加该影视类型与该影视之间的边， 关系为 CATEGORY_IN\n",
    "        gn.add_edge(row['title'], cat, label=\"CATEGORY_IN\")\n",
    "    \n",
    "    # 遍历涉及的国家列表\n",
    "    for cou in row['countries']:\n",
    "        \n",
    "        # 添加国家节点\n",
    "        gn.add_node(cou, label=\"COUNTRY\")\n",
    "        \n",
    "        # 添加该国家与该影视之间的边， 关系为 COUNTRY_IN\n",
    "        gn.add_edge(row['title'], cou, label=\"COUNTRY_IN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [14,14]\n",
    "\n",
    "\n",
    "\n",
    "def get_adjacent_nodes(G, nodes):\n",
    "    sub_graph=set()\n",
    "    for n in nodes:\n",
    "        sub_graph.add(n)\n",
    "        for e in G.neighbors(n):        \n",
    "            sub_graph.add(e)\n",
    "    return list(sub_graph)\n",
    "\n",
    "def draw_sub_graph(G, sub_graph):\n",
    "    \n",
    "    # 从图网络 G 中取出子图 sub_graph\n",
    "    subgraph = G.subgraph(sub_graph)\n",
    "    pos = nx.spring_layout(subgraph)\n",
    "    \n",
    "    # 为每一种图节点标注一种颜色和大小\n",
    "    node_colors=[]\n",
    "    node_sizes = []\n",
    "    for n in subgraph.nodes():\n",
    "        if G.nodes[n]['label'] == \"MOVIE\":\n",
    "            node_colors.append('blue')\n",
    "            node_sizes.append(700)\n",
    "        elif G.nodes[n]['label'] == \"PERSON\":\n",
    "            node_colors.append('red')\n",
    "            node_sizes.append(600)\n",
    "        elif G.nodes[n]['label'] == \"CATEGORY\":\n",
    "            node_colors.append('green')\n",
    "            node_sizes.append(500)\n",
    "        elif G.nodes[n]['label'] == \"COUNTRY\":\n",
    "            node_colors.append('yellow')\n",
    "            node_sizes.append(400)\n",
    "            \n",
    "    nx.draw(subgraph, pos, with_labels=True, node_color=node_colors, node_size=node_sizes, width=2, font_size=15)\n",
    "    \n",
    "    # 给每一条边绘制 label\n",
    "    edge_labels = {}\n",
    "    for e in subgraph.edges():\n",
    "        if G.edges[e]['label'] == \"ACTED_IN\":\n",
    "            edge_labels[e] = \"ACTED_IN\"\n",
    "        if G.edges[e]['label'] == \"DERECTED_BY\":\n",
    "            edge_labels[e] = \"DERECTED_BY\"\n",
    "        if G.edges[e]['label'] == \"CATEGORY_IN\":\n",
    "            edge_labels[e] = \"CATEGORY_IN\"\n",
    "        if G.edges[e]['label'] == \"COUNTRY_IN\":\n",
    "            edge_labels[e] = \"COUNTRY_IN\"\n",
    "    nx.draw_networkx_edge_labels(subgraph, pos, edge_labels=edge_labels, font_color=\"red\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [\"Ocean's Twelve\", \"Ocean's Thirteen\"]\n",
    "sub_graph = get_adjacent_nodes(gn, nodes)\n",
    "\n",
    "draw_sub_graph(gn, sub_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [\"Superman Returns\", \"Tom and Jerry: The Magic Ring\"]\n",
    "sub_graph = get_adjacent_nodes(gn, nodes)\n",
    "\n",
    "draw_sub_graph(gn, sub_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = nx.shortest_path(gn, \"Superman Returns\", \"Tom and Jerry: The Magic Ring\")\n",
    "print(\"最短路径图节点：\")\n",
    "print(path)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"最短路径图节点以及它们直接的关系：\")\n",
    "for i in range(len(path) - 1):\n",
    "    print(\"{}\\t{}\\t{}\".format(path[i], path[i+1], gn.edges[(path[i], path[i+1])][\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四步理解图网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install node2vec\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "df = pd.read_csv('/home/jovyan/kernel/netflix_titles.csv')\n",
    "\n",
    "# 取出导演属性，如果该属性的值为空，则返回空列表 [] ，否则返回所有导演列表\n",
    "df['directors'] = df['director'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\n",
    "\n",
    "# 取出演员属性，如果该属性的值为空，则返回空列表 [] ，否则返回所有演员列表\n",
    "df['actors'] = df['cast'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\n",
    "\n",
    "# 取出导演属性，如果该属性的值为空，则返回空列表 [] ，否则返回所有影视类型列表\n",
    "df['categories'] = df['listed_in'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\n",
    "\n",
    "# 取出国家属性，如果该属性的值为空，则返回空列表 [] ，否则返回所有国家列表\n",
    "df['countries'] = df['country'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\n",
    "\n",
    "# 取出我们需要的5个属性\n",
    "df = df[[\"title\", \"directors\", \"actors\", \"categories\", \"countries\"]]\n",
    "\n",
    "\n",
    "# 初始化一个图网络 graph network (gn)\n",
    "gn = nx.Graph(label=\"Netflix\")\n",
    "\n",
    "# 遍历数据来给图网络添加节点和边\n",
    "for i, row in df.iterrows():\n",
    "    \n",
    "    # 添加影视节点\n",
    "    gn.add_node(row['title'], label=\"MOVIE\")\n",
    "    \n",
    "    # 遍历演员列表\n",
    "    for actor in row['actors']:\n",
    "        \n",
    "        # 添加人物节点\n",
    "        gn.add_node(actor, label=\"PERSON\")\n",
    "        \n",
    "        # 添加该人物与该影视之间的边，关系为 ACTED_IN\n",
    "        gn.add_edge(row['title'], actor, label=\"ACTED_IN\")\n",
    "    \n",
    "    # 遍历导演列表\n",
    "    for director in row['directors']:\n",
    "        \n",
    "        # 添加人物节点\n",
    "        gn.add_node(director, label=\"PERSON\")\n",
    "        \n",
    "        # 添加该人物与该影视之间的边， 关系为 DERECTED_BY\n",
    "        gn.add_edge(row['title'], director, label=\"DERECTED_BY\")\n",
    "    \n",
    "    # 遍历影视类型列表\n",
    "    for cat in row['categories']:\n",
    "        \n",
    "        # 添加影视类型节点\n",
    "        gn.add_node(cat, label=\"CATEGORY\")\n",
    "        \n",
    "        # 添加该影视类型与该影视之间的边， 关系为 CATEGORY_IN\n",
    "        gn.add_edge(row['title'], cat, label=\"CATEGORY_IN\")\n",
    "    \n",
    "    # 遍历涉及的国家列表\n",
    "    for cou in row['countries']:\n",
    "        \n",
    "        # 添加国家节点\n",
    "        gn.add_node(cou, label=\"COUNTRY\")\n",
    "        \n",
    "        # 添加该国家与该影视之间的边， 关系为 COUNTRY_IN\n",
    "        gn.add_edge(row['title'], cou, label=\"COUNTRY_IN\")\n",
    "\n",
    "        \n",
    "# Node2Vec\n",
    "\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "n2v = Node2Vec(gn, dimensions=100, walk_length=16, num_walks=10)\n",
    "model = n2v.fit(window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate similiar movies to given genre or title\n",
    "def print_similiar(name):\n",
    "    for node, _ in model.most_similar(name):\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# # 定义 TSNE，映射至2维空间\n",
    "tsne = TSNE(n_components=2)\n",
    "# # 从训练好的 Node2Vec 中取出 Node Embedding\n",
    "node_embeddings = model.wv.vectors\n",
    "node_embeddings_2d = tsne.fit_transform(node_embeddings)\n",
    "\n",
    "# 在画图前，我们将每一个节点按 label 区分不同的颜色\n",
    "node_ids = model.wv.index2word \n",
    "node_labels = [gn.nodes[node_id][\"label\"] for node_id in node_ids]\n",
    "label_map = {\n",
    "    \"MOVIE\":\"blue\",\n",
    "    \"PERSON\":\"red\",\n",
    "    \"CATEGORY\":\"green\",\n",
    "    \"COUNTRY\":\"yellow\"\n",
    "}\n",
    "\n",
    "node_colours = [ label_map[lab] for lab in node_labels]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(node_embeddings_2d[:,0], \n",
    "            node_embeddings_2d[:,1], \n",
    "            c=node_colours, cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"MOVIE\":\"blue\",\n",
    "    \"PERSON\":\"red\",\n",
    "    \"CATEGORY\":\"green\",\n",
    "    \"COUNTRY\":\"yellow\"\n",
    "}\n",
    "\n",
    "\n",
    "def draw_n2v_sub_graph(nodes, markers):\n",
    "    '''\n",
    "    nodes: list of string，影视作品名称列表\n",
    "    markers: list of string, 用不同的标记绘制不同的影视作品\n",
    "    '''\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    for node, marker in zip(nodes, markers):\n",
    "        \n",
    "        sub_graph_emb_x = []\n",
    "        sub_graph_emb_y = []\n",
    "        sub_graph_title = []\n",
    "        sub_graph_targets = []\n",
    "        color_cls = []\n",
    "        for n in get_adjacent_nodes(gn, [node]):\n",
    "            sub_graph_title.append(n)\n",
    "            n_id = model.wv.index2word.index(n)\n",
    "            sub_graph_emb_x.append(node_embeddings_2d[n_id, 0])\n",
    "            sub_graph_emb_y.append(node_embeddings_2d[n_id, 1])\n",
    "            sub_graph_targets.append(gn.nodes[n][\"label\"])\n",
    "            color_cls.append(label_map[gn.nodes[n][\"label\"]])\n",
    "\n",
    "\n",
    "        ax.scatter(sub_graph_emb_x, sub_graph_emb_y, s=300, c=color_cls, marker=marker, cmap=\"jet\", label=node)\n",
    "\n",
    "        for emb_x, emb_y, emb_title in zip(sub_graph_emb_x, sub_graph_emb_y, sub_graph_title):\n",
    "            ax.annotate(emb_title, (emb_x, emb_y), textcoords=\"offset points\", xytext=(0,10), ha='center', size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_n2v_sub_graph([\"Ocean's Twelve\", \"Ocean's Thirteen\"], [\"o\", \"d\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_n2v_sub_graph([\"Superman Returns\", \"Tom and Jerry: The Magic Ring\"], [\"o\", \"d\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## node2vec 模型搭建影视作品推荐系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_recommendation(G, root, label):\n",
    "\t# 在图G中，使用 Adamic/Adar 公式，根据root节点以及label标签进行推荐\n",
    "\t# 使用字典记录共同邻居，字典中的key，value表示root与key的共同邻居是value\n",
    "    commons_dict = {}\n",
    "\t# 遍历root节点附近的邻居e1\n",
    "    for e1 in G.neighbors(root):\n",
    "\t    # 遍历e1附近的邻居e2\n",
    "        for e2 in G.neighbors(e1):\n",
    "\t\t # 假如e2为root节点本身，则跳过\n",
    "            if e2 == root:\n",
    "                continue\n",
    "\t\t\t# 判断e2节点的标签是否符合需求\n",
    "            if G.nodes[e2]['label'] == label:\n",
    "\t\t\t    # 将e1加入 root与e2 的共同邻居集合\n",
    "                commons = commons_dict.get(e2)\n",
    "                if commons==None:\n",
    "                    commons_dict.update({e2 : [e1]})\n",
    "                else:\n",
    "                    commons.append(e1)\n",
    "                    commons_dict.update({e2 : commons})\n",
    "    nodes =[]\n",
    "    weight=[]\n",
    "\t# 根据Adamic/Adar公式计算相似度\n",
    "    for key, values in commons_dict.items():\n",
    "        w = 0.0\n",
    "        for e in values:\n",
    "            w = w + 1 / math.log(G.degree(e))\n",
    "        nodes.append(key) \n",
    "        weight.append(w)\n",
    "    # 按相似度从高到低排序返回结果\n",
    "    result = pd.Series(data=np.array(weight), index=nodes)\n",
    "    result.sort_values(inplace=True, ascending=False)        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据Ocean's Twelve推荐一批电影\n",
    "recommends = get_recommendation(gn, \"Ocean's Twelve\", label=\"MOVIE\")\n",
    "print(\"*\"*40+\"\\n Recommendation for 'Ocean's Twelve'\\n\"+\"*\"*40)\n",
    "print(recommends.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommends = get_recommendation(gn, \"Superman Returns\", label=\"MOVIE\")\n",
    "print(\"*\"*40+\"\\n Recommendation for 'Superman Returns'\\n\"+\"*\"*40)\n",
    "print(recommends.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommends = get_recommendation(gn, \"Tom and Jerry: The Magic Ring\", label=\"MOVIE\")\n",
    "print(\"*\"*40+\"\\n Recommendation for 'Tom and Jerry: The Magic Ring'\\n\"+\"*\"*40)\n",
    "print(recommends.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommends = get_recommendation(gn, \"Brad Pitt\", label=\"PERSON\")\n",
    "print(\"*\"*40+\"\\n Recommendation for 'Brad Pitt'\\n\"+\"*\"*40)\n",
    "print(recommends.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation_by_n2v(name):\n",
    "    for node, _ in model.most_similar(name):\n",
    "        print(node)\n",
    "\n",
    "get_recommendation_by_n2v(\"Ocean's Twelve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format(\"./models/netflix_n2v\")\n",
    "model.save(\"./models/netflix_n2v_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于图卷积神经网络的图节点分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
    "! tar -xf cora.tgz\n",
    "\n",
    "! ls cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 将数据读入dataFrame数据结构\n",
    "raw_data = pd.read_csv('cora/cora.content',sep = '\\t',header = None)\n",
    " # 样本点数2708\n",
    "num = raw_data.shape[0]\n",
    "print(\"样本点数\", num)\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('论文id：', raw_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一篇论文的bag of words向量\n",
    "raw_data.iloc[0:1, 1:1434]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('论文出现词语数量：', raw_data.iloc[0:1, 1:1434].sum(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('论文类型：', raw_data.iloc[0, 1434])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 论文引用数据\n",
    "raw_data_cites = pd.read_csv('cora/cora.cites',sep = '\\t',header = None)\n",
    "raw_data_cites[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    \"\"\"\n",
    "    将标签变为onehot向量\n",
    "    \"\"\"\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def load_data(path=\"./cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"\n",
    "    按行对稀疏矩阵进行归一化\n",
    "    \"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"\n",
    "    将一个稀疏矩阵从scipy格式转化为torch格式\n",
    "    \"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    \"\"\"\n",
    "    准确率计算方法\n",
    "    \"\"\"\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content数据转换为numpy向量\n",
    "idx_features_labels = np.array(raw_data)\n",
    "# 将每篇论文的词袋向量取出作为每篇文章的特征向量并存储为稀疏矩阵格式\n",
    "features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "# 将每篇论文的类型取出作为label并转换成one hot向量\n",
    "labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "# 取出每篇论文的id\n",
    "idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "# 将论文id映射到[0, 2708这个区间]\n",
    "idx_map = {j: i for i, j in enumerate(idx)}\n",
    "\n",
    "# cites数据转换为numpy向量\n",
    "edges_unordered = np.array(raw_data_cites)\n",
    "# 将cites数据中的id映射到[0, 2708这个区间]\n",
    "edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                 dtype=np.int32).reshape(edges_unordered.shape)\n",
    "# 将论文间的引用关系存储成稀疏矩阵格式\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                    shape=(labels.shape[0], labels.shape[0]),\n",
    "                    dtype=np.float32)\n",
    "\n",
    "# 构建对称的邻接矩阵\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "# 对文章的特征进行归一化\n",
    "features = normalize(features)\n",
    "# 【先将邻接矩阵加上一个单位矩阵，然后对其进行归一化】对邻接矩阵进行归一化\n",
    "adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "# 产出最终的向量\n",
    "idx_train = range(140)\n",
    "idx_val = range(200, 500)\n",
    "idx_test = range(500, 1500)\n",
    "\n",
    "features = torch.FloatTensor(np.array(features.todense()))\n",
    "labels = torch.LongTensor(np.where(labels)[1])\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    图卷积的一个简单实现，具体可以参考论文 https://arxiv.org/abs/1609.02907\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    in_features : int\n",
    "        图卷积输入特征向量的大小，即 $|H^{(l)}|$\n",
    "    out_features : int\n",
    "        图卷积输出向量的大小，即 $|H^{(l+1)}|$\n",
    "    bias : bool\n",
    "        是否使用偏置向量，默认为 True，即默认是使用偏置向量\n",
    "    weight: Parameter\n",
    "        图卷积中可训练的参数，\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    __init__(self, in_features, out_features, bias=True)\n",
    "        图卷积的构造函数，定义输入特征的大小，输出向量的大小，是否使用偏置，参数\n",
    "    reset_parameters(self)\n",
    "        初始化图卷积中的参数\n",
    "    forward(self, input, adj)\n",
    "        前向传播函数，input 是特征输入，adj 是变换后的邻接矩阵 $N(A)=D^{-1}\\tilde{A}$。完成前向传播的计算逻辑，$N(A) H^{(l)} W^{(l)}$\n",
    "    __repr__(self)\n",
    "        重构类名表达\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # H * W\n",
    "        support = torch.mm(input, self.weight)\n",
    "        # N(A) * H * W\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            # N(A) * H * W + b\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    '''\n",
    "    两层图卷积神经网络模型\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_feat : int\n",
    "        图网络输入特征向量的大小\n",
    "    n_hid : int\n",
    "        隐藏层维度大小，即第一层图卷积层的输出向量的大小\n",
    "    n_class : int\n",
    "        分类器类别数量\n",
    "    dropout: float\n",
    "        dropout 率\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    __init__(self, n_feat, n_hid, n_class, dropout)\n",
    "        两层图卷积神经网络构造函数，定义输入 feature 的维度，隐藏层维度，分类器类别数量，dropout 率\n",
    "    forward(self, x, adj)\n",
    "        前向传播函数，x 是图网络输入 feature，adj 是已经变换过的邻接矩阵 $N(A)$\n",
    "    '''\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        # 定义第一层图卷积层，输入是图网络 feature，维度是 n_feat，输出维度是 n_hid\n",
    "        self.gc1 = GraphConvolution(n_feat, n_hid)\n",
    "        # 定义第二层图卷积层，输入是第一层的输出向量，维度是 n_hid，输出是分类器在各个类别上的概率\n",
    "        self.gc2 = GraphConvolution(n_hid, n_class)\n",
    "        # 定义熟悉 dropout 率\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # 第一层卷积层的输出，并经过非线性激活函数 Relu 的输出\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        # dropout\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # 第二层卷积层的输出，映射到输出类别维度\n",
    "        x = self.gc2(x, adj)\n",
    "        # 计算 log softmax\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# 训练超参配置\n",
    "class Args:\n",
    "    no_cuda = False     # 是否使用 cuda/gpu\n",
    "    seed = 42           # 设置随机种子\n",
    "    epochs = 200        # 迭代次数\n",
    "    lr = 0.01           # 学习率\n",
    "    weight_decay = 5e-4 # 学习率衰减\n",
    "    hidden = 16         # 隐藏层维度\n",
    "    dropout = 0.5       # dropout 率\n",
    "\n",
    "\n",
    "\n",
    "args = Args()\n",
    "# 是否使用 gpu/cuda\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "# 设置随机种子\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# 加载数据，包括变换后的邻接矩阵，图网络输入 feature，分类标签，训练数据，验证数据，测试数据\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "\n",
    "# 利用定义好的两层图卷积神经网络模型来构造一个 GCN 实例，\n",
    "# 图网络输入 feature 维度为 features.shape[1]\n",
    "# 隐藏层维度为 args.hidden\n",
    "# 分类输出类别数量为 labels.max().item() + 1\n",
    "# dropout 率为 args.dropout\n",
    "model = GCN(n_feat=features.shape[1],\n",
    "            n_hid=args.hidden,\n",
    "            n_class=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "\n",
    "# 构造一个 Adam 优化器，\n",
    "# 需要优化的参数是 GCN 模型里的可训练参数\n",
    "# 学习率设置为 args.lr\n",
    "# 学习率衰减是 args.weight_decay\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, \n",
    "                       weight_decay=args.weight_decay)\n",
    "\n",
    "\n",
    "# 模型训练函数，epoch 为第几次迭代\n",
    "def train(epoch):\n",
    "    # 记录第 epoch 次迭代的开始时间\n",
    "    t = time.time()\n",
    "    # 标记 GCN 模型处于 train mode\n",
    "    model.train()\n",
    "    # 在每一个 epoch 都需要先清空之前计算过的梯度\n",
    "    optimizer.zero_grad()\n",
    "    # 将图网络输入 feature 和变换后的邻接矩阵 adj 输入至两层图卷积神经网络 GCN 模型中，经过前向传播得到输出，该输出即为在分类类别上的预测概率\n",
    "    output = model(features, adj)\n",
    "    # 根据训练集的数据索引找到对应的输出概率和标签，由此计算损失 loss，以及准确率\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    # 误差反向传播\n",
    "    loss_train.backward()\n",
    "    # 优化器开始进行优化 GCN 中的可训练参数\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    # 利用验证集数据对该 epoch 训练结果进行验证。验证过程需要关闭train mode 并打开 eval model\n",
    "    model.eval()\n",
    "    # 同样进行前向传播\n",
    "    output = model(features, adj)\n",
    "    # 根据验证集的数据索引找到对应的输出概率和标签，由此计算损失 loss，以及准确率\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    # 打印所有的结果，以及所需要的时间\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "# 记录模型训练开始的时间\n",
    "t_start = time.time()\n",
    "# 开始迭代训练 GCN 模型，迭代次数设置为 args.epochs\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "    \n",
    "print(\"模型训练完成！\")\n",
    "print(\"模型训练总耗时: {:.4f}s\".format(time.time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型测试函数\n",
    "def test():\n",
    "    # 首先标记模型为 eval mode\n",
    "    model.eval()\n",
    "    # 将图网络输入 feature 和变换后的邻接矩阵 adj 输入至两层图卷积神经网络 GCN 模型中，经过前向传播得到输出，该输出即为在分类类别上的预测概率\n",
    "    output = model(features, adj)\n",
    "    # 根据测试集的数据索引找到对应的输出概率和标签，由此计算损失 loss，以及准确率\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    # 打印测试结果\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    \n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于图卷积神经网络的链接预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "```\n",
    "```python\n",
    "def encode_onehot(labels):\n",
    "    \"\"\"\n",
    "    将标签变为onehot向量\n",
    "    \"\"\"\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"\n",
    "    将scipy稀疏矩阵转换成torch稀疏矩阵\n",
    "    \"\"\"\n",
    "    # 取出矩阵中每个不为0的值的坐标\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    # 坐标转化为numpy格式\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    # 取出矩阵中的值，转化为torch格式\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    # 获得矩阵的形状，转化为torch格式\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    # 返回torch格式的稀疏矩阵\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"\n",
    "    将scipy稀疏矩阵转换成三元组，包括（坐标，值，矩阵形状）\n",
    "    \"\"\"\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    \"\"\"\n",
    "    图的预处理，包括归一化操作\n",
    "    \"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    return sparse_mx_to_torch_sparse_tensor(adj_normalized)\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    \"\"\"\n",
    "    数据集划分，随机将10%的边作为测试集\n",
    "    \"\"\"\n",
    "\n",
    "    # 将对角线上的值去掉\n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "\n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 10.))\n",
    "    num_val = int(np.floor(edges.shape[0] / 20.))\n",
    "\n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "\n",
    "    def ismember(a, b, tol=5):\n",
    "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "\n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], edges_all):\n",
    "            continue\n",
    "        if test_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                continue\n",
    "        test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], val_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], val_edges):\n",
    "            continue\n",
    "        if val_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                continue\n",
    "        val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    assert ~ismember(test_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges, train_edges)\n",
    "    assert ~ismember(test_edges, train_edges)\n",
    "    assert ~ismember(val_edges, test_edges)\n",
    "\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "\n",
    "    # 重新构建邻接矩阵\n",
    "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 将数据读入dataFrame数据结构\n",
    "raw_data = pd.read_csv('cora/cora.content', sep='\\t', header=None)\n",
    "# 样本点数2708\n",
    "num = raw_data.shape[0]\n",
    "raw_data_cites = pd.read_csv('cora/cora.cites', sep='\\t', header=None)\n",
    "# content数据转换为numpy向量\n",
    "idx_features_labels = np.array(raw_data)\n",
    "# 将每篇论文的词袋向量取出作为每篇文章的特征向量并存储为稀疏矩阵格式\n",
    "features = sp.csr_matrix(idx_features_labels[:, 1: -1], dtype=np.float32)\n",
    "\n",
    "# 取出每篇论文的id\n",
    "idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "# 将论文id映射到[0, 2708这个区间]\n",
    "idx_map = {j: i for i, j in enumerate(idx)}\n",
    "\n",
    "# cites数据转换为numpy向量\n",
    "edges_unordered = np.array(raw_data_cites)\n",
    "# 将cites数据中的id映射到[0, 2708这个区间]\n",
    "edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                 dtype=np.int32).reshape(edges_unordered.shape)\n",
    "\n",
    "label = encode_onehot(idx_features_labels[:, -1])\n",
    "# 将论文间的引用关系存储成稀疏矩阵格式\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                    shape=(label.shape[0], label.shape[0]),\n",
    "                    dtype=np.float32)\n",
    "\n",
    "features = torch.FloatTensor(np.array(features.todense()))\n",
    "adj = nx.adjacency_matrix(nx.convert_matrix.from_scipy_sparse_matrix(adj))\n",
    "\n",
    "# 把原始的邻接矩阵存下来方便后面评测用\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "# 图的预处理\n",
    "adj_norm = preprocess_graph(adj)\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "\n",
    "# 计算引用与引用两个关系之间的比值，用于后续训练\n",
    "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "# 正则项，用于后续训练\n",
    "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN4Link(nn.Module):\n",
    "    '''\n",
    "    两层图卷积神经网络模型\n",
    "    ...\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_feat : int\n",
    "        图网络输入特征向量的大小\n",
    "    n_hid : int\n",
    "        隐藏层维度大小\n",
    "    n_class : int\n",
    "        分类器类别数量\n",
    "    dropout: float\n",
    "        dropout 率\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__(self, n_feat, n_hid, n_class, dropout)\n",
    "        两层图卷积神经网络构造函数，定义输入 feature 的维度，隐藏层维度，分类器类别数量，dropout 率\n",
    "    forward(self, x, adj)\n",
    "        前向传播函数，x 是图网络输入 feature，adj 是已经变换过的邻接矩阵 $N(A)$\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_feat, n_hid, dropout):\n",
    "        super(GCN4Link, self).__init__()\n",
    "        # 定义第一层图卷积层，输入是图网络 feature，维度是 n_feat，输出维度是 n_hid\n",
    "        self.gc1 = GraphConvolution(n_feat, n_hid)\n",
    "        # 定义第二层图卷积层，输入维度是 n_hid，输出维度是 n_hid\n",
    "        self.gc2 = GraphConvolution(n_hid, n_hid, dropout)\n",
    "        # 定义熟悉 dropout 率\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # 第一层卷积层的输出，并经过非线性激活函数 Relu 的输出\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        # dropout\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # 第二层卷积层的输出，映射到输出类别维度\n",
    "        x = self.gc2(x, adj)\n",
    "\n",
    "        # dropout增加模型鲁棒性 \n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        # 基于点乘计算每两个节点之间存在边的概率\n",
    "        adj_preds = torch.mm(x, x.t())\n",
    "        return adj_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练超参配置\n",
    "class Args:\n",
    "    no_cuda = False     # 是否使用 cuda/gpu\n",
    "    seed = 42           # 设置随机种子\n",
    "    epochs = 500        # 迭代次数\n",
    "    lr = 0.01           # 学习率\n",
    "    hidden = 64         # 隐藏层维度\n",
    "    dropout = 0.       # dropout 率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def get_acc_score(emb, adj_orig, edges_pos, edges_neg):\n",
    "    \"\"\"\n",
    "    评测预测的准确率\n",
    "    \"\"\"\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    # 得到每一个正例的预测分数\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "    # 得到每一个负例的预测分数\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "    # 合并预测分数与标准答案，使用sklearn.metrics里面自带的正确率评价器评测指标\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "    acc_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import time\n",
    "\n",
    "args = Args()\n",
    "# 是否使用 gpu/cuda\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "# 设置随机种子\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "# 初始化模型\n",
    "model = GCN4Link(n_feat=features.shape[1],\n",
    "        n_hid=args.hidden,\n",
    "        dropout=args.dropout)\n",
    "\n",
    "# 定义模型优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(args.epochs):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # 前向传播\n",
    "    preds = model(features, adj_norm)\n",
    "    # 使用binary_cross_entropy计算loss，使用之前计算好的norm与pos_weight调节 引用边 与 非引用边 的数量\n",
    "    loss = norm * F.binary_cross_entropy_with_logits(preds, adj_label, pos_weight=torch.tensor(pos_weight))\n",
    "\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    cur_loss = loss.item()\n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "\n",
    "    # 在验证集上测试模型效果\n",
    "    acc_curr = get_acc_score(preds.data.numpy(), adj_orig, val_edges, val_edges_false)\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cur_loss),\n",
    "          \"accurancy=\", \"{:.5f}\".format(acc_curr),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t)\n",
    "          )\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# 在测试集上测试模型效果\n",
    "acc = get_acc_score(preds.data.numpy(), adj_orig, test_edges, test_edges_false)\n",
    "print('Test accurancy: ' + str(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
